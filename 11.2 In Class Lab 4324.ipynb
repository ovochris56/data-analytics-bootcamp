{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Feature Engineering\n",
    "# Create a binary target variable: 1 if Profit is above average, 0 otherwise\n",
    "average_profit = df['Profit'].mean()\n",
    "df['Target'] = (df['Profit'] > average_profit).astype(int)\n",
    "\n",
    "# Data Preprocessing\n",
    "# Separating the feature variables (X) and the target variable (y)\n",
    "X = df.drop(['Profit', 'Target'], axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Handling missing values and converting categorical data\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Create transformers for numerical and categorical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that first preprocesses the data then applies the classifier\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))  # Increase max_iter if convergence warning\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57792d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision trees\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Handling missing values\n",
    "# Create transformers for numerical and categorical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Separating the feature variables (X) and the target variable (y)\n",
    "# Assuming the target variable is named 'Target'\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that first preprocesses the data then applies the classifier\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Classification Report:\\n{classification_report(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Separating the feature variables (X) and the target variable (y)\n",
    "# Assuming 'Target' is the name of the target variable\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Handling missing values\n",
    "# Create transformers for numerical and categorical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that first preprocesses the data then applies the classifier\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Classification Report:\\n{classification_report(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df596c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Separating the feature variables (X) and the target variable (y)\n",
    "# Assuming 'Target' is the name of the target variable\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Handling missing values and scaling numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())  # SVMs assume that data is scaled\n",
    "])\n",
    "\n",
    "# Converting categorical variables using one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that preprocesses the data and then applies the classifier\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(kernel='linear'))  # You can choose other kernels like 'rbf'\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Classification Report:\\n{classification_report(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Nearest Neighbor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Separating the feature variables (X) and the target variable (y)\n",
    "# Assuming 'Target' is the name of the target variable\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Feature Scaling is crucial for KNN as it is a distance-based algorithm\n",
    "# Scaling numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Converting categorical variables using one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that preprocesses the data and then applies the classifier\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=5))  # n_neighbors can be tuned\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Classification Report:\\n{classification_report(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ed44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Assuming 'Target' is the name of the target variable and the rest are features\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Handling missing values and scaling numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())  # Optional: scale features if necessary\n",
    "])\n",
    "\n",
    "# Converting categorical variables using one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that preprocesses the data and then applies the regressor\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'R-squared (R^2): {r2_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Regression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Feature Engineering\n",
    "# Selecting variables for which we want to create polynomial features\n",
    "# For example, let's assume we are interested in 'Feature1' and 'Feature2' for polynomial features\n",
    "selected_features = ['Feature1', 'Feature2']\n",
    "\n",
    "# Generating polynomial and interaction features\n",
    "# Degree of polynomial can be adjusted\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Data Preprocessing\n",
    "# Assuming 'Target' is the name of the target variable\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns excluding those selected for polynomial features\n",
    "numerical_features = [col for col in X.select_dtypes(include=['int64', 'float64']).columns.tolist() if col not in selected_features]\n",
    "\n",
    "# Handling missing values and scaling numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Converting categorical variables using one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that preprocesses the data, generates polynomial features, and then applies the regressor\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'R-squared (R^2): {r2_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge and Lasso Regression \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Assuming 'Target' is the name of the target variable and the rest are features\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Handling missing values and scaling numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())  # It's good practice to scale features for regularization\n",
    "])\n",
    "\n",
    "# Converting categorical variables using one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model Training: Ridge Regression\n",
    "ridge_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1.0))  # Alpha can be tuned\n",
    "])\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Model Training: Lasso Regression\n",
    "lasso_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Lasso(alpha=0.1))  # Alpha can be tuned\n",
    "])\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation: Ridge Regression\n",
    "ridge_pred = ridge_model.predict(X_test)\n",
    "print(\"Ridge Regression\")\n",
    "print(f'Mean Squared Error (MSE): {mean_squared_error(y_test, ridge_pred)}')\n",
    "print(f'R-squared (R^2): {r2_score(y_test, ridge_pred)}')\n",
    "\n",
    "# Evaluation: Lasso Regression\n",
    "lasso_pred = lasso_model.predict(X_test)\n",
    "print(\"\\nLasso Regression\")\n",
    "print(f'Mean Squared Error (MSE): {mean_squared_error(y_test, lasso_pred)}')\n",
    "print(f'R-squared (R^2): {r2_score(y_test, lasso_pred)}')\n",
    "\n",
    "# Observe the coefficients\n",
    "ridge_coefs = ridge_model.named_steps['regressor'].coef_\n",
    "lasso_coefs = lasso_model.named_steps['regressor'].coef_\n",
    "\n",
    "print(\"\\nRidge coefficients:\", ridge_coefs)\n",
    "print(\"Lasso coefficients:\", lasso_coefs)\n",
    "\n",
    "# Discuss the effect of regularization\n",
    "print(\"\\nEffect of Regularization:\")\n",
    "print(\"Ridge regression minimizes coefficient size squared (L2 penalty).\")\n",
    "print(\"Lasso regression minimizes coefficient size (L1 penalty) and can set some coefficients to zero, thus performing feature selection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means Clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Handling missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_filled = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Normalizing numerical features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_filled), columns=df_filled.columns)\n",
    "\n",
    "# Clustering with KMeans\n",
    "# Trying with a range of clusters to find the optimal one\n",
    "inertia = []\n",
    "cluster_options = [2, 3, 4, 5, 6]  # Replace with the range of clusters you want to try\n",
    "for n_clusters in cluster_options:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the inertia to see which number of clusters is best\n",
    "plt.plot(cluster_options, inertia, '-o')\n",
    "plt.xlabel('Number of clusters, k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Inertia of k-Means versus number of clusters')\n",
    "plt.show()\n",
    "\n",
    "# Let's assume the optimal number of clusters is 3 for this example\n",
    "optimal_clusters = 3\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)\n",
    "df_scaled['Cluster'] = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# Visualization & Interpretation\n",
    "# Reduce the dimension to 2D using PCA and visualize the clusters\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_scaled.drop('Cluster', axis=1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_scaled['Cluster'], cmap='viridis', marker='o', edgecolor='k', s=50)\n",
    "plt.title('2D Visualization of Clusters')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Analyzing and interpreting the characteristics of each cluster\n",
    "# Describe each cluster\n",
    "for i in range(optimal_clusters):\n",
    "    cluster_data = df[df_scaled['Cluster'] == i]\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    print(cluster_data.describe().transpose())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierarchial Clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Identifying categorical columns with dtype 'object'\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "# Identifying numerical columns\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Handling missing values and scaling numerical features\n",
    "# Converting categorical variables using one-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "df_processed = preprocessor.fit_transform(df)\n",
    "df_processed = pd.DataFrame(df_processed)\n",
    "\n",
    "# Clustering\n",
    "# Use the linkage method to perform the hierarchical clustering\n",
    "linked = linkage(df_processed, method='ward')\n",
    "\n",
    "# Visualization: Displaying the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,\n",
    "           orientation='top',\n",
    "           labels=np.array(df.index),\n",
    "           distance_sort='descending',\n",
    "           show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Interpreting the dendrogram\n",
    "print(\"Interpret the dendrogram by observing the height at which any two clusters merge. Larger heights indicate that clusters are more distinct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace 'data.csv' with your actual CSV file path\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Assuming all features are numerical. If there are categorical features,\n",
    "# consider encoding them or excluding them from clustering.\n",
    "# Normalizing the features\n",
    "scaler = StandardScaler()\n",
    "df_normalized = scaler.fit_transform(df)\n",
    "\n",
    "# Clustering with DBSCAN\n",
    "# Experiment with different values of epsilon and min_samples\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters = dbscan.fit_predict(df_normalized)\n",
    "\n",
    "# Visualization & Interpretation\n",
    "# Plotting the clusters and noise points\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Clusters are marked with different colors, noise points are marked with black\n",
    "unique_labels = set(clusters)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "    \n",
    "    class_member_mask = (clusters == k)\n",
    "    \n",
    "    xy = df_normalized[class_member_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14 if k == -1 else 6)\n",
    "\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.show()\n",
    "\n",
    "# Discuss the characteristics of the identified dense regions\n",
    "print(\"Clusters identified by DBSCAN are highlighted, and noise points are marked in black.\")\n",
    "print(\"Each cluster represents a dense region of data points surrounded by a region of lower density.\")\n",
    "print(\"Points not assigned to any cluster, and thus considered noise, are those in sparse areas.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
